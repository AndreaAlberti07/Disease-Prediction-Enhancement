\section{ML Model Methodology}
This section provides a comprehensive overview of the methodologies employed in the construction of the machine 
learning model. The discussion encompasses various techniques designed to handle the intricacies of model building, 
coupled with a logical flow that guides the entire process.

% ------------------- Data Preparation -------------------

\subsection{Data Balancing}
% - Face the unbalanced dataset problem



% ------------------- Feature Extraction -------------------

\subsection{Feature Extraction}
% - Prepare the features and normalization
A pivotal phase in constructing a machine learning model is feature extraction. In addition to the one-hot vector representation 
of symptoms, the network analysis affords us the following features:

\begin{itemize}
    \setlength\itemsep{0.4em} % set space between items
    \item \textbf{L1 and L2 Measures}: A vector with values representing the L1 and L2 measures for each symptom.
    \item \textbf{Betweenness Centrality}: A vector with values denoting the betweenness centrality of each symptom.
    \item \textbf{Community Count}: A vector indicating the number of symptoms belonging to each community.
    \item \textbf{Community Size}: A vector replacing symptoms with the size of the community to which they belong.
\end{itemize}
\vspace{0.4cm}

Given the diverse scales of these features, normalization becomes imperative for their cohesive integration into the model 
without introducing biases. To achieve this, we opted for \textit{MaxAbs} normalization. This normalization scales each feature 
individually, ensuring that the maximal absolute value of each feature in the training set becomes 1.0, while preserving the sparsity of data.


% ------------------- Operative Flow -------------------

\subsection{Operative Flow}
% - Operative Flow
	% - select best parameters for symptoms one hot only
	% - select best parameters for combination of other features (best combination is chosen with random parameters looking at the accuracy)
	% - train for each model the two version above with optimal parameters
	% - pick the best model according to accuracy
	% - train the best model with whole dataset and reduce the number of features

Once the features are ready, the core part of the model-building process can begin. 
We trained three different models: a Logistic Regression, a Random Forest, and a Multi-Layer Perceptron (MLP).

For each model, we faced the challenge of selecting both the best parameters and the most effective features. 
The interdependence between these two aspects makes the optimal approach to explore all the possible combination of features
and for each combination trying all the parameters combination. This approach is not feasible in terms of computational effort
leading us to adopt a greedy approach. We firstly split the features into two 
groups: the symptoms' one-hot vector and the remaining features. The former is used to train a base model, 
while the latter is utilized to explore the potential improvement brought by the new features.

Using Algorithm \ref{alg:feature_selection}, we determined the best feature combination for each group (symptoms and other features).
Subsequently, given the optimal feature combination, we identified the best parameter combination using Algorithm 
\ref{alg:best_selection}. Each model was then trained with the best parameters and the best features combination, 
and the model with the highest accuracy was chosen. Finally, the best overall model was trained with the entire dataset.


\begin{algorithm}[H] \small
	\caption{Feature Selection Algorithm}\label{alg:feature_selection}
	
	\begin{algorithmic}[1]
	
	\State BestFeatureComb $\gets$ EmptySet
	
	\For{each model}
	    \State CurrentModel $\gets$ EmptyModel
	    \State BestModAccuracy $\gets$ 0
	    \State Parameters $\gets$ InitializeRandomParameters
	
	    \For{i = 1 to NumberOfFeatures}
	    		\State BestAccuracy $\gets$ -1

			\For{each feature}
				\State TrainModel(CurrentModel, Parameters)
			
				\State CurrentAccuracy $\gets$ GetAccuracy(CurrentModel)
			
				\If{CurrentAccuracy $>=$ BestAccuracy}
					\State BestAccuracy $\gets$ CurrentAccuracy
					\State BestFeature $\gets$ feature
					
				\EndIf
				\State UpdateModel(CurrentModel, BestFeature)
			\EndFor
			\State FreezeFeatures(CurrentModel)
			\State ModelAccuracy[i] $\gets$ GetAccuracy(CurrentModel)
			\State FeaturesComb[i] $\gets$ GetFeat(CurrentModel)
	
	    \EndFor
	
	    \State BestComb $\gets$ FeaturesComb[Max(ModelAccuracy)]
	    \State BestFeatureComb $\gets$ BestComb $\cup$ BestFeatureComb
	
	\EndFor
	
	\State \textbf{return} BestFeatureComb
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] \small
	\caption{Best Model Selection Algorithm}\label{alg:best_selection}
	
	\begin{algorithmic}[1]
	
	\State CurrentAccuracy $\gets$ 0
	\For{each model}
	    
	    \State CreateGrid(Parameters)
	    \State BestParameters $\gets$ GridSearchCV(model)
	    \State TrainModel(model, BestParameters)
	    \State CurrentAccuracy $\gets$ GetAccuracy(model)

	    \If{CurrentAccuracy $>=$ BestAccuracy}
			\State BestAccuracy $\gets$ CurrentAccuracy
			\State BestModel $\gets$ model
			\State BestParameters $\gets$ Parameters
	    \EndIf

	\EndFor

	\State FullDataTrain(BestModel, BestParameters)
	\State BestAccuracy $\gets$ GetAccuracy(BestModel)
	\State ReduceFeatures(BestModel, BestParameters)
	
	\State \textbf{return} BestParameters, BestModel, BestAccuracy
	\end{algorithmic}
\end{algorithm}


At the conclusion of these procedures, we obtained the following two models:\\

\begin{itemize}
    \setlength\itemsep{0.4em} % set space between items
    \item \textbf{Symptoms Model}: The best model with the optimal parameters and the symptoms as features
    \item \textbf{Other Features Model}: The best model with the optimal parameters and the best features combination
\end{itemize}
\vspace{0.4cm}

With these models in hand, we could compare their prediction performance to evaluate whether the network 
features contributed to an accuracy improvement. Additionally, we applied the feature reduction technique discussed 
in Section \ref{sec:feature_reduction} to both models, assessing whether network features could reduce computational 
complexity without compromising accuracy.

