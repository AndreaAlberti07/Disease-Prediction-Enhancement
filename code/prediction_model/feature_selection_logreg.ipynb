{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-wise Feature selection (Network only)\n",
    "Forward step-wise selection of the best features to use, following a greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from matplotlib import colors\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"features/\"\n",
    "names = [\"betweenness\", \"community_count\", \"community_size\", \"ks_level_1\", \"ks_level_2\"]\n",
    "np.random.seed(42)\n",
    "samples_per_class = 35\n",
    "model = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from .npz files\n",
    "original_labels = np.load(path + names[0] + \".npz\")['y']\n",
    "original_features = np.load(path + names[0] + \".npz\")['X']\n",
    "columns = [original_features.shape[1]]\n",
    "for filename in names[1:]:\n",
    "    file_path = path + filename + \".npz\"\n",
    "    data = np.load(file_path)\n",
    "    feature_matrix = data['X']\n",
    "    columns.append(feature_matrix.shape[1])\n",
    "    original_features = np.concatenate((original_features, feature_matrix), axis=1)\n",
    "\n",
    "# Over-sample\n",
    "original_samples_per_class = {label: np.sum(original_labels == label) for label in np.unique(original_labels)}\n",
    "sampling_strategy = {label: max(samples_per_class, original_samples) for label, original_samples in original_samples_per_class.items()}\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "oversampled_features, oversampled_labels = ros.fit_resample(original_features, original_labels)\n",
    "# Under-sample\n",
    "updated_samples_per_class = {label: np.sum(oversampled_labels == label) for label in np.unique(original_labels)}\n",
    "sampling_strategy = {label: min(samples_per_class, original_samples) for label, original_samples in updated_samples_per_class.items()}\n",
    "rus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "undersampled_features, labels = rus.fit_resample(oversampled_features, oversampled_labels)\n",
    "\n",
    "# Build features list\n",
    "features = []\n",
    "current = 0\n",
    "for c in columns:\n",
    "    feature = undersampled_features[:, current:current + c]\n",
    "    features.append(feature)\n",
    "    current += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection and record accuracies\n",
    "num_features = len(features)\n",
    "remaining_features = list(range(num_features))\n",
    "already_used_features = []\n",
    "accuracies = []\n",
    "\n",
    "null_model_accuracy = 0\n",
    "\n",
    "# Feature selection iterations\n",
    "it = 0\n",
    "accuracy_matrix = np.zeros((num_features, num_features))\n",
    "while remaining_features:\n",
    "    best_accuracy = 0\n",
    "    best_feature = None\n",
    "\n",
    "    for feature_index in remaining_features:\n",
    "        current_features = already_used_features + [feature_index]\n",
    "        X = np.hstack([features[i] for i in current_features])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "        # Perform logistic regression and compute accuracy\n",
    "        model_ = clone(model)\n",
    "        model_.fit(X_train, y_train)\n",
    "        y_pred = model_.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        accuracy_matrix[it][feature_index] = accuracy\n",
    "        print(f\"Model [{it}][{feature_index}]: accuracy = {accuracy*100:4.2f}.\")\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_feature = feature_index\n",
    "\n",
    "    # Update lists\n",
    "    accuracies.append(best_accuracy)\n",
    "    remaining_features.remove(best_feature)\n",
    "    already_used_features.append(best_feature)\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_used_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model accuracies with custom colormap\n",
    "norm = colors.Normalize(vmin=0, vmax=1)\n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(accuracy_matrix, cmap=cmap, norm=norm)\n",
    "tick_marks = np.arange(num_features)\n",
    "reordered_names = [names[i] for i in already_used_features]\n",
    "plt.xticks(tick_marks, labels=names, rotation='vertical', ha='center')  # Set custom tick marks\n",
    "plt.yticks(tick_marks, labels=reordered_names)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xlabel(\"Features to be added\")\n",
    "plt.ylabel(\"Best feature\")\n",
    "\n",
    "# Add text with colored background based on the value\n",
    "for i in range(num_features):\n",
    "    for j in range(num_features):\n",
    "        val = accuracy_matrix[i, j]\n",
    "        color = cmap(norm(val))\n",
    "        plt.text(j, i, f\"{val*100:4.2f}\", ha='center', va='center', color='red' if val == 0 else 'black', backgroundcolor=color)\n",
    "\n",
    "plt.title(f\"Accuracy of the partial models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies for each best model\n",
    "plt.plot(np.arange(1, len(accuracies)+1), np.array(accuracies)*100, marker='o')\n",
    "plt.xlabel('Number of Metrics')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Accuracy vs. Number of Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection based on Commonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on a specific criterion\n",
    "\n",
    "# Compute the threshold\n",
    "threshold_feature = \"ks_level_2\"\n",
    "\n",
    "thr_data = features[names.index(threshold_feature)]\n",
    "\n",
    "percentiles = np.percentile(np.mean(thr_data, axis=1), np.geomspace(50, 1e-6, 5), axis=0)\n",
    "\n",
    "used_features = copy.deepcopy([features[i] for i in already_used_features[:np.argmax(accuracies)]])\n",
    "\n",
    "accuracies_filt = []\n",
    "percentages = []\n",
    "for perc in percentiles:\n",
    "    filtered_coefficients = np.where(np.mean(thr_data, axis=0) > perc)[0]\n",
    "    if filtered_coefficients.size == 0:\n",
    "        continue\n",
    "    X = np.hstack([f[:, filtered_coefficients] if f.shape[1] >= filtered_coefficients.max() + 1 else f for f in used_features])\n",
    "    data_percentage = filtered_coefficients.shape[0]/thr_data.shape[1]\n",
    "\n",
    "    # Perform logistic regression and compute accuracy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "    model_ = clone(model)\n",
    "    model_.fit(X_train, y_train)\n",
    "    y_pred = model_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy = {accuracy*100:4.2f}; retained features = {data_percentage*100:4.2f}%\")\n",
    "    accuracies_filt.append(accuracy)\n",
    "    percentages.append(data_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies for each model\n",
    "plt.plot(np.array(percentages)*100, np.array(accuracies_filt)*100, marker='o')\n",
    "plt.axhline(y=np.max(accuracies)*100, color='r', linestyle='--', label='')\n",
    "plt.plot(100, np.max(accuracies)*100, color='r', marker='o')\n",
    "plt.xlabel('% of features retained')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 100)\n",
    "plt.xlim(10, 101)\n",
    "plt.title('Accuracy vs. Filtered features, based on '+threshold_feature)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection based on Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on a specific criterion\n",
    "\n",
    "# Compute the threshold\n",
    "threshold_feature = \"ks_level_1\"\n",
    "\n",
    "thr_data = features[names.index(threshold_feature)]\n",
    "\n",
    "percentiles = np.percentile(np.mean(thr_data, axis=1), np.linspace(50, 1e-6, 5), axis=0)\n",
    "\n",
    "used_features = copy.deepcopy([features[i] for i in already_used_features[:np.argmax(accuracies)]])\n",
    "\n",
    "accuracies_filt = []\n",
    "percentages = []\n",
    "for perc in percentiles:\n",
    "    filtered_coefficients = np.where(np.mean(thr_data, axis=0) > perc)[0]\n",
    "    if filtered_coefficients.size == 0:\n",
    "        continue\n",
    "    X = np.hstack([f[:, filtered_coefficients] if f.shape[1] >= filtered_coefficients.max() + 1 else f for f in used_features])\n",
    "    data_percentage = filtered_coefficients.shape[0]/thr_data.shape[1]\n",
    "\n",
    "    # Perform logistic regression and compute accuracy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "    model_ = clone(model)\n",
    "    model_.fit(X_train, y_train)\n",
    "    y_pred = model_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy = {accuracy*100:4.2f}; retained features = {data_percentage*100:4.2f}%\")\n",
    "    accuracies_filt.append(accuracy)\n",
    "    percentages.append(data_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies for each model\n",
    "plt.plot(np.array(percentages)*100, np.array(accuracies_filt)*100, marker='o')\n",
    "plt.axhline(y=np.max(accuracies)*100, color='r', linestyle='--', label='')\n",
    "plt.plot(100, np.max(accuracies)*100, color='r', marker='o')\n",
    "plt.xlabel('% of features retained')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 100)\n",
    "plt.xlim(10, 101)\n",
    "plt.title('Accuracy vs. Filtered features, based on '+threshold_feature)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
