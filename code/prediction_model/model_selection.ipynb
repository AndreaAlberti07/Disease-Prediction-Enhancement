{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jl\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy, precision, recall and AUPRC for each model\n",
    "def compute_metrics(model, data):\n",
    "    y_pred = model.predict(data[:, :-1])\n",
    "    y_true = data[:, -1]\n",
    "    accuracy = sk.metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = sk.metrics.precision_score(y_true, y_pred)\n",
    "    recall = sk.metrics.recall_score(y_true, y_pred)\n",
    "    auprc = sk.metrics.average_precision_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models With Only Symptoms One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "path = '../prediction_model/models/'\n",
    "names = ['log_reg_classsic', 'random_forest_classic', 'MLP_classic']\n",
    "\n",
    "log_reg = jl.load(path + names[0] + '.joblib')\n",
    "random_forest = jl.load(path + names[1] + '.joblib')\n",
    "MLP = jl.load(path + names[2] + '.joblib')\n",
    "\n",
    "# Load the data\n",
    "path = '../prediction_model/features/'\n",
    "names = ['symptoms']\n",
    "\n",
    "symptoms = np.load(path + names[0] + '.npz')\n",
    "data = np.concatenate((symptoms['X'], symptoms['y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with model names and metrics and the corresponding values\n",
    "metrics = ['accuracy', 'precision', 'recall', 'AUPRC']\n",
    "df = pd.DataFrame(columns=metrics)\n",
    "df['model'] = names\n",
    "df.set_index('model', inplace=True)\n",
    "\n",
    "df.loc['log_reg', metrics] = compute_metrics(log_reg, data)\n",
    "df.loc['random_forest', metrics] = compute_metrics(random_forest, data)\n",
    "df.loc['MLP', metrics] = compute_metrics(MLP, data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracies\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(names, df['accuracy'])\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_title('Accuracy of the models')\n",
    "plt.show()\n",
    "\n",
    "# plot precision and recall, using two axis\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(names, df['precision'], color='red')\n",
    "ax.set_ylabel('precision', color='red')\n",
    "ax.tick_params(axis='y', labelcolor='red')\n",
    "ax.set_title('Precision and recall of the models')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(names, df['recall'], color='blue')\n",
    "ax2.set_ylabel('recall', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "plt.show()\n",
    "\n",
    "# plot AUPRC\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(names, df['AUPRC'])\n",
    "ax.set_ylabel('AUPRC')\n",
    "ax.set_title('AUPRC of the models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models With New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "path = '../prediction_model/models/'\n",
    "names = ['log_reg_mix', 'random_forest_mix', 'MLP_mix']\n",
    "\n",
    "log_reg = jl.load(path + names[0] + '.joblib')\n",
    "random_forest = jl.load(path + names[1] + '.joblib')\n",
    "MLP = jl.load(path + names[2] + '.joblib')\n",
    "\n",
    "# Load the data\n",
    "path = '../prediction_model/features/'\n",
    "names = ['betweenness', 'community_count', 'community_size']\n",
    "\n",
    "betweenness = np.load(path + names[0] + '.npz')\n",
    "community_count = np.load(path + names[1] + '.npz')\n",
    "community_size = np.load(path + names[2] + '.npz')\n",
    "\n",
    "# Associate the features \n",
    "data_log_reg = np.concatenate((betweenness['X'], community_count['X'], community_size['X'], community_count['y']), axis=1)\n",
    "data_random_forest = np.concatenate((betweenness['X'], community_count['X'], community_size['X'], community_count['y']), axis=1)\n",
    "data_MLP = np.concatenate((community_count['X'], community_size['X'], community_count['y']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with model names and metrics and the corresponding values\n",
    "metrics = ['accuracy', 'precision', 'recall', 'AUPRC']\n",
    "df = pd.DataFrame(columns=metrics)\n",
    "df['model'] = names\n",
    "df.set_index('model', inplace=True)\n",
    "\n",
    "df.loc['log_reg', metrics] = compute_metrics(log_reg, data_log_reg)\n",
    "df.loc['random_forest', metrics] = compute_metrics(random_forest, data_random_forest)\n",
    "df.loc['MLP', metrics] = compute_metrics(MLP, data_MLP)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracies\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(names, df['accuracy'])\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_title('Accuracy of the models')\n",
    "plt.show()\n",
    "\n",
    "# plot precision and recall, using two axis\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(names, df['precision'], color='red')\n",
    "ax.set_ylabel('precision', color='red')\n",
    "ax.tick_params(axis='y', labelcolor='red')\n",
    "ax.set_title('Precision and recall of the models')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(names, df['recall'], color='blue')\n",
    "ax2.set_ylabel('recall', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "plt.show()\n",
    "\n",
    "# plot AUPRC\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(names, df['AUPRC'])\n",
    "ax.set_ylabel('AUPRC')\n",
    "ax.set_title('AUPRC of the models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the two best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "path = '../prediction_model/models/'\n",
    "names = ['log_reg_classsic', 'random_forest_mix']\n",
    "\n",
    "classic = jl.load(path + names[0] + '.joblib')\n",
    "mix = jl.load(path + names[1] + '.joblib')\n",
    "\n",
    "# Load the data\n",
    "path = '../prediction_model/features/'\n",
    "names = ['symptoms', 'betweenness', 'community_count', 'community_size']\n",
    "\n",
    "symptoms = np.load(path + names[0] + '.npz')\n",
    "betweenness = np.load(path + names[1] + '.npz')\n",
    "community_count = np.load(path + names[2] + '.npz')\n",
    "community_size = np.load(path + names[3] + '.npz')\n",
    "\n",
    "data_classic = np.concatenate((symptoms['X'], symptoms['y']), axis=1)\n",
    "data_mix = np.concatenate((betweenness['X'], community_count['X'], community_size['X'], community_count['y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with model names and metrics and the corresponding values\n",
    "metrics = ['accuracy', 'precision', 'recall', 'AUPRC']\n",
    "df = pd.DataFrame(columns=metrics)\n",
    "df['model'] = names\n",
    "df.set_index('model', inplace=True)\n",
    "\n",
    "df.loc['log_reg', metrics] = compute_metrics(log_reg, data_log_reg)\n",
    "df.loc['random_forest', metrics] = compute_metrics(random_forest, data_random_forest)\n",
    "df.loc['MLP', metrics] = compute_metrics(MLP, data_MLP)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracies\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(names, df['accuracy'])\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_title('Accuracy of the models')\n",
    "plt.show()\n",
    "\n",
    "# plot precision and recall, using two axis\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(names, df['precision'], color='red')\n",
    "ax.set_ylabel('precision', color='red')\n",
    "ax.tick_params(axis='y', labelcolor='red')\n",
    "ax.set_title('Precision and recall of the models')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(names, df['recall'], color='blue')\n",
    "ax2.set_ylabel('recall', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "plt.show()\n",
    "\n",
    "# plot AUPRC\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(names, df['AUPRC'])\n",
    "ax.set_ylabel('AUPRC')\n",
    "ax.set_title('AUPRC of the models')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
